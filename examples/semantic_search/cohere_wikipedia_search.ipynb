{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55683ea8",
   "metadata": {},
   "source": [
    "# Building a RAG chain from Wikipedia\n",
    "\n",
    "This notebook shows how to use ApertureDB as part of a Retrieval-Augmented Generation Langchain pipeline.  This means that we're going to use ApertureDB as a vector-based search engine to find documents that match the query and then use a large-language model to generate an answer based on those documents. \n",
    "\n",
    "We already have a corpus of >600k paragraphs from the Simple English Wikipedia with associated embeddings provided by Cohere.\n",
    "(If not, see [Ingesting Wikipedia into ApertureDB](./cohere_wikipedia_ingest.ipynb)).\n",
    "We'll use that to answer natural-language questions.\n",
    "\n",
    "![RAG workflow](images/RAG_Demo.png)\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8188f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet aperturedb langchain langchain-core langchain-community langchainhub langchain-cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd257925-73ab-4d76-a20a-708a255285d2",
   "metadata": {},
   "source": [
    "## Choose a prompt\n",
    "\n",
    "The prompt ties together the source documents and the user's query, and also sets some basic parameters for the chat engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f3be8f-04ec-4376-886b-f15c0dca3cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following documents to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.  Additionally, you should always indicate which documents support each part of your answer.\n",
      "Question: {question} \n",
      "{context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following documents to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.  Additionally, you should always indicate which documents support each part of your answer.\n",
    "Question: {question} \n",
    "{context} \n",
    "Answer:\"\"\")\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29e93b-aefd-4f6f-9b70-e3bbab075426",
   "metadata": {},
   "source": [
    "For comparison, we're also going to ask the same questions of the language model without using documents.  This prompt is for a non-RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919d7974-dd0f-4069-8a2b-9ee1b412f467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Answer the question from your general knowledge.  If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt2 = PromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Answer the question from your general knowledge.  If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Answer:\"\"\")\n",
    "print(prompt2.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9bfa05",
   "metadata": {},
   "source": [
    "# Cohere API Key\n",
    "\n",
    "In order to continue with this demo, you will need to enter an API key for Cohere.\n",
    "An evaluation API key can be obtained for free from [dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7861bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['COHERE_API_KEY'] = getpass()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c818d-d549-4ce0-86d3-cd6201911a10",
   "metadata": {},
   "source": [
    "## Select an embedding scheme\n",
    "\n",
    "Here we select the embedding scheme that matches the embeddings we have preloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f416fd1-7b42-4a56-9af1-dbc54f75bf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0030612946, 0.046173096, 0.024490356, 0.032440186, -0.028900146, -0.026855469, -0.02810669, -0.03074646, -0.068481445, 0.033966064] 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "embeddings = CohereEmbeddings(model=\"embed-multilingual-v3.0\")\n",
    "\n",
    "emb = embeddings.embed_query(\"Hello, world!\")\n",
    "print(emb[:10], len(emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f69082-c220-4cc0-a29a-23a1c9721b58",
   "metadata": {},
   "source": [
    "## Select a vectorstore\n",
    "\n",
    "Here we're using an instance of ApertureDB that has already been pre-loaded with a selection of paragraphs from Wikipedia.\n",
    "\n",
    "First activate the connection to ApertureDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f8c4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ['APERTUREDB_JSON'] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282933d",
   "metadata": {},
   "source": [
    "## Create vectorstore\n",
    "\n",
    "Now we create a LangChain vectorstore object, backed by the ApertureDB instance we have already uploaded documents to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3ff395e-8e09-4e24-b96d-af3a9f3007f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import ApertureDB\n",
    "import logging\n",
    "import sys\n",
    "# date_strftime_format = \"%Y-%m-%y %H:%M:%S\"\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.WARN, \n",
    "#                     format=\"%(asctime)s %(levelname)s %(funcName)s %(message)s\", datefmt=date_strftime_format)\n",
    "\n",
    "DESCRIPTOR_SET = \"cohere_wikipedia_2023_11_embed_multilingual_v3\"\n",
    "\n",
    "vectorstore = ApertureDB(embeddings=embeddings, \n",
    "                 descriptor_set=DESCRIPTOR_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c9537-eeff-411d-9a85-68b2e186d730",
   "metadata": {},
   "source": [
    "## Create a retriever\n",
    "\n",
    "The retriever is responsible for finding the most relevant documents in the vectorstore for a given query.  Here's we using the \"max marginal relevance\" retriever, which is a simple but effective way to find a diverse set of documents that are relevant to a query.  For each query, we retrieve the top 10 documents, but we do so by fetching 20 and then selecting the top 5 using the MMR algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf7e4bf2-c22c-496f-8b70-8adafc0ad649",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_type = \"mmr\" # \"similarity\" or \"mmr\"\n",
    "k = 10              # number of results used by LLM\n",
    "fetch_k = 100       # number of results fetched for MMR\n",
    "retriever = vectorstore.as_retriever(search_type=search_type, \n",
    "    search_kwargs=dict(k=k, fetch_k=fetch_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33d881-d5d1-442d-af01-70330ac6b736",
   "metadata": {},
   "source": [
    "## Select an LLM engine\n",
    "\n",
    "Here we're again using Cohere, but there's no need to use the same provider as we used for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1da3f60-d278-4e95-8ced-59a4fa2c558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "llm = ChatCohere(model=\"command-r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815917b3-339c-447c-b1e9-a692f8462ae3",
   "metadata": {},
   "source": [
    "## Build the chain\n",
    "\n",
    "Now we put it all together.  The chain is responsible for taking a user query and returning a response.  It does this by first retrieving the most relevant documents using vector search, then using the LLM to generate a response.\n",
    "\n",
    "For demonstration purposes, we're printing the documents that were retrieved, but in a real application you would probably want to hide this information from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83ad132e-8804-43d5-8da4-f852dd42d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"Document {i}: \" + doc.page_content for i, doc in enumerate(docs, start=1))\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc6ccf-ab84-4213-b931-71a757321e91",
   "metadata": {},
   "source": [
    "This chain does not use RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dfd155b-4a04-4db7-b08c-f74a2d82c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_chain = (\n",
    "  {\"question\": RunnablePassthrough()}\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee70edc",
   "metadata": {},
   "source": [
    "## Look at some documents\n",
    "\n",
    "In order to come up with questions that match the corpus, it might be a good idea to look at some random documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d5ae3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Family: Foster families are families where a child lives with and is cared for by people who are not his or her biological parents.\n",
      "2. Flag of the United States: When a new state joins the United States, a new flag is made with an extra star. The new flag is first flown on the 4th of July (Independence Day).\n",
      "3. Arithmetic: Most people learn arithmetic in primary school, but some people do not learn arithmetic and others forget the arithmetic they learned. Many jobs require a knowledge of arithmetic, and many employers complain that it is hard to find people who know enough arithmetic. A few of the many jobs that require arithmetic include carpenters, plumbers, mechanics, accountants, architects, doctors, and nurses. Arithmetic is needed in all areas of mathematics, science, and engineering.\n",
      "4. Prison: There are many books and poems about prisons or prison life, such as The Count of Monte Cristo by Alexandre Dumas, père and The Ballad of Reading Gaol by Oscar Wilde.\n",
      "5. European Union: In 1993, with the Treaty of Maastricht it changed its name to the European Union. Now the member countries work together not only in politics and economy (coal, steel and trade), but also in money, justice (laws), and foreign affairs. With the Schengen Agreement, 22 member countries of the EU opened their borders to each other, so people can now travel from one country to the other without a passport or identity card. Many of the countries of the EU also share a currency, which is called the euro. 10 new countries became members of the EU in 2004, 2 more became members in 2007, and 1 more in 2013. In 2016, the United Kingdom voted to leave the EU. Today there are 27 member countries altogether.\n",
      "6. Metal: Some metals are used to make items like coins because they are hard and will not wear away quickly. For example, copper (which is shiny and red in color), aluminium (which is shiny and white), gold (which is yellow and shiny), and silver and nickel (also white and shiny).\n",
      "7. Banana: Some people are allergic to bananas. There are two basic forms of these allergies. The first is known as oral allergy syndrome. Within an hour of eating a banana, swelling starts inside the mouth or throat. This allergy is related to allergies caused by pollen, like that of the birch tree. The other is similar to latex allergies. It causes urticaria and potentially serious upper gastrointestinal symptoms.\n",
      "8. Eye: Like different cameras, different eyes have different abilities. They may have higher or lower resolution, the ability to detect small details. They may have different performance in low light; nocturnal animals can see better at night than daytime animals. They may have different ability to distinguish colours.\n",
      "9. Scientific method: Does sugar dissolve faster in hot water or cold water? Does the temperature affect how fast the sugar dissolves? This is a question we might want to ask.\n",
      "10. Science: Science makes models of nature, models of our universe, and medicine. There are many different sciences with their own names. However it is not right to say \"science says\" any one thing. Science is a process, not just the facts and rules believed at one time.\n"
     ]
    }
   ],
   "source": [
    "from aperturedb.CommonLibrary import create_connector\n",
    "offset = 0\n",
    "query = [ {\"FindDescriptor\": {\"set\": DESCRIPTOR_SET, \"results\": { \"list\": [\"text\", \"lc_title\"], \"limit\": 10}, \"offset\": offset, \"sort\": { \"key\": \"uniqueid\" } }} ]\n",
    "client = create_connector()\n",
    "response, _ = client.query(query)\n",
    "for i, result in enumerate(list(response[0].values())[0][\"entities\"], start=1):\n",
    "    print(f\"{i}. {result['lc_title']}: {result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bbb54-b743-4d24-b506-57c125ab2bc0",
   "metadata": {},
   "source": [
    "## Run the chain\n",
    "\n",
    "Now we can enter a query and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f0cf16d-3c62-40ce-86ec-c12692b646be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a67a1c7524475c885c313e4ea55572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Combobox(value='', continuous_update=False, options=('What is the answer to the great question of life, the un…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f621df55aa64b37a37f1db9f857ea4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Combobox, Output, Button\n",
    "from IPython.display import display_markdown, Markdown, display, clear_output\n",
    "\n",
    "queries = [\n",
    "\"What is the answer to the great question of life, the universe, and everything?\",\n",
    "\"What is the largest city in France\",\n",
    " \"What is the queen of citadels in Lille?\",\n",
    "\"What is the Elephant's Graveyard?\",\n",
    "\"What can you see along the A61 in England?\",\n",
    "\"What person has a connection with the city of Leicester?\",\n",
    "\"What are the ancient wards of the city of London?\",\n",
    "\"Who created mini marvels?\",\n",
    "\"What happens at the end of Moby Dick?\",\n",
    "\"What is the Euclidean Algorithm used for?\",\n",
    "\"Can I use an EV to power my house?\",\n",
    "\"Who is the main character in The Glass Castle?\",\n",
    "\"What did Winston Smith do for work?\",\n",
    "\"What schools are in Great Bend, Kansas?\",\n",
    "\"Who was the first Catholic nominee for U.S. President for a major party?\",\n",
    "\"What are the inside and outside styles of the Azerbaijan State Philharmonic Hall?\",\n",
    "\"What were the sides in the Mali War?\",\n",
    "\"What is the population of Kingston, New Hampshire?\",\n",
    "\"What did Jacques Ellul say about mass media communication?\",\n",
    "\"Who played Lucas Sinclair?\",\n",
    "\"What is stannic oxide used for?\",\n",
    "]\n",
    "\n",
    "def handler(event):\n",
    "    if event.name != 'value' or input.value == \"\":\n",
    "        return\n",
    "        \n",
    "    user_query = input.value\n",
    "    input.value = \"\"\n",
    "\n",
    "    with output:\n",
    "        clear_output()\n",
    "        run_query(user_query)\n",
    "    \n",
    "def run_query(user_query):\n",
    "    display(Markdown(f\"### User Query\\n{user_query}\"))\n",
    "        \n",
    "    nonrag_answer = plain_chain.invoke(user_query)\n",
    "    display(Markdown(f\"### Non-RAG Answer\\n{nonrag_answer}\"))\n",
    "\n",
    "    rag_answer = rag_chain_with_source.invoke(user_query)\n",
    "    display(Markdown(\"\\n\".join([\n",
    "        f\"### RAG Answer\\n{rag_answer['answer']}\",\n",
    "        f\"### Documents\",\n",
    "        *(f\"{i}. **[{doc.metadata['title']}]({doc.metadata['url']})**: {doc.page_content}\" for i, doc in enumerate(rag_answer[\"context\"], 1) )\n",
    "        ])))\n",
    "\n",
    "\n",
    "interactive = True\n",
    "if interactive:\n",
    "    input = Combobox(\n",
    "        placeholder=\"Enter a question...\",\n",
    "        options = queries,\n",
    "        ensure_option=False,\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "    )\n",
    "    \n",
    "    output = Output()\n",
    "        \n",
    "    input.observe(handler)\n",
    "    \n",
    "    display(input)\n",
    "    display(output)    \n",
    "else: # For testing non-interactively\n",
    "    user_query = queries[0]\n",
    "    run_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced20785-616c-4d54-9af8-629884433a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
